### NLP自然语言处理任务

### LSTM网络

长短期记忆网络是一种循环神经网络（RNN）的变体，它能够有效地处理和预测时间序列数据，如文本、语音、视频等，LSTM网络通过专门的记忆单元来解决RNN中的梯度消失和爆炸问题，使其能够更好地捕捉长期依赖关系，被广泛应用于自然语言处理、时间序列预测等领域。

### PLM预训练语言模型

通过预训练一个双向LSTM网络来捕捉上下文感知的词的表示，然后根据特定的下游任务微调LSTM网络。这些预训练的上下文感知词表示作为通用语义特征，极大的提高了NLP（自然语言处理任务）任务的性能。

### LLM大语言模型

扩展PLM的数据量或者模型大小会提高下游任务的模型性能。通常，LLM 是指包含数千亿（或更多）参数的 Transformer 语言模型

### LLaMA大语言模型

LLaMA-13B 在大多数基准测试中都优于 GPT-3 (175B)，LLaMA65B 与最佳模型 Chinchilla-70B 和 PaLM-540B 相比具有竞争力。

**背景：模型参数量级的积累，或者训练数据的增加，哪个对性能提升帮助更大？**

大家好像有一个共识，就是：**模型参数量级的增加就会带来同样的性能提升。**

但是事实确实如此吗？

根据**缩放定律**：

>训练大语言模型时，在计算成本达到最优情况下，模型大小和训练数据 (token) 的数量应该比例相等地缩放，即：如果模型的大小加倍，那么训练数据的数量也应该加倍。

翻译过来就是：当我们给定特定的计算成本预算的前提下，语言模型的最佳性能不仅仅可以通过设计较大的模型搭配小一点的数据集得到，也可以通过设计较小的模型配合大量的数据集得到。

那么，**相似成本训练 LLM，是大 LLM 配小数据训练，还是小 LLM** **配大数据训练更好？**

根据**LLaMA: Open and Efficient Foundation Language Models**这篇论文得出，作者认为，大部分人是用已经训练好的LLM进行推理的，所以，我们首选的模型应该不是训练最快的，而是推理最快的LLM。

所以，对于用已经训练好的LLM来说，小LLM配大数据训练更好，因为小LLM推理更友好。

> 推理：通常指的是使用已经训练好的模型对新的数据进行预测或者分类。
>
> 推理是指给定一个输入文本，模型会生成与该输入相关的文本序列。这对话题生成、机器翻译等任务都具有重要意义。在这种情况下，推理可被视为模型在给定输入后产生输出的过程。

[LLaMA]([LLM 系列超详细解读 (六)：LLaMa：开源高效的大语言模型 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/643894722))

### 开发LLM

现有的一种可行方法是在现有的LLM的基础上进行开发，

训练良好的模型检查点对于开展LLM的开发和研究至关重要

可以使用公开的API执行推理任务

### 预训练规模中的token

在自然语言处理中，预训练模型中的 "token" 指的是文本被分割成的基本单元。

这些基本单元可以是单词、子词（如词根或词缀的一部分），甚至是字符。

在预训练模型中，这些 token 成为模型处理的基本单位，在对文本进行编码和解码时起到关键作用。

"token" 数量指的是在模型的词汇表（vocabulary）中存在的不同 token 的数量。

### 百亿参数级别的模型

> 百亿参数量级别的模型通常指的是模型参数数量在数百亿级别的大型神经网络模型。这种规模的模型通常被用于处理极其复杂的任务，如自然语言处理中的机器翻译、语言生成、文本理解等。这类模型需要大量的计算资源来进行训练和推断，通常适用于大规模数据集和高要求的任务。

举例来说，OpenAI的GPT-3模型就是一个典型的百亿参数级别的模型，百亿参数级别的模型在处理自然语言理解和生成任务时，通常能够更好地捕捉语言的复杂结构和语义信息，从而在各种NLP任务中取得更好的表现。

#### 大模型的参数规模怎么选

选择大型语言模型的参数规模通常需要权衡多个因素：

1. **任务需求**：模型的参数规模应该与任务的复杂性和规模相匹配。对于简单的任务，小型模型可能已经足够，但对于复杂的自然语言处理任务，可能需要更大规模的模型来取得更好的性能。
2. **计算资源**：大型模型需要更多的计算资源来训练和推断。参数规模应该在可用的计算资源范围内，否则会导致训练时间很长或推理速度很慢。
3. **数据集大小**：大型模型需要大量的数据来进行训练，因此参数规模的选择也应该考虑可用的训练数据量。
4. **性能要求**：模型的参数规模通常会影响其性能。通常来说，更大的模型往往可以带来更好的性能，但这也取决于具体的任务。
5. **可解释性**：较大规模的模型可能会降低可解释性，因此需要在模型性能和可解释性之间进行权衡。

#### CoT提示数据

CoT进行微调是指用T5模型进行微调

以便根据特定的文本到文本任务来调整模型，使其更好地适应特定的应用场景或任务

### 千亿参数级别的模型

现在来看，百亿参数是模型具备涌现能力的门槛，千亿参数的模型具备较好的涌现能力,当模型参数量增长超过一定阈值时，模型能力表现出跃迁式的提升，表现出来语言理解能力、生成能力、逻辑推理能力等能力的显著提升，这也就是我们所说的模型的涌现能力。但这并不意味着模型规模就要上升到万亿规模级别的竞争，因为现有大模型并没有得到充分训练，如 GPT-3 的每个参数基本上只训练了 1-2 个Token，DeepMind 的研究表明，如果把一个大模型训练充分，需要把每个参数量训练 20 个 Token。

**所以，当前的很多千亿规模的大模型还需要用多 10 倍的数据进行训练，模型性能才能达到比较好的水平。**


#### 大模型API接口的选择

在选择大型模型的API接口时，应该考虑以下几个关键因素：

1. **功能和任务需求**：确保选用的API接口能够满足你的具体任务需求。例如，有些接口可能专注于文本生成，而另一些则更适合于文本分类或问答系统。
2. **性能和准确度**：评估不同接口的性能和准确度，可以通过对比测试或查看相关性能报告来了解。
3. **支持的语言和功能**：确保API接口支持你所需的语言，并且具备适合你任务的特定功能（比如对多语言的支持、特定领域的专业知识等）。
4. **成本和价格**：考虑不同API接口的成本和定价模式。有些接口可能按照调用次数计费，而另一些可能提供不同的订阅模式。
5. **可扩展性**：如果你的应用需要处理大量请求或需要高度可扩展性，那么选择一个具有良好扩展性和稳定性的API接口是非常重要的。
6. **技术支持和文档**：选择提供全面技术支持和详细文档的API接口，这将有助于在实施过程中遇到问题时进行解决。

### 训练集的选择

[现存的大语言模型数据集有哪些？ - 知乎 (zhihu.com)](https://www.zhihu.com/question/601247782)

[Zjh-819/LLMDataHub：趋势指令微调数据集的快速指南（尤其是） (github.com)](https://github.com/Zjh-819/LLMDataHub)

### 常用语料库

#### BookCorpus

由Google发布的一个大规模图书文本语料库

包含了大约11,038本英文书籍的文本数据，总计超过7亿个单词。

更好地探索模型的表现、语言结构、主题建模等方面

#### CommonCrawl

该数据集由对互联网进行定期抓取和存档而成，内容涵盖了从网页文本到各类多媒体资源的广泛内容

可以用于构建语料库、训练模型或挖掘有关互联网内容的见解

**CommonCrawl获取的网络数据中存在的大量噪音和低质量信息如何处理**

[LLM Data Pipelines: 解析大语言模型训练数据集处理的复杂流程 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/645451072)

[GPT-3 训练语料 Common Crawl 处理流程 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/610659484)

1. **数据清洗**：首先，对抓取的网页内容进行数据清洗。这可能包括去除HTML标记、特殊字符，解决乱码问题，以及统一格式等。这可以通过正则表达式、HTML解析库、以及Unicode编码方面的处理来完成。
2. **停用词过滤和词频处理**：进行停用词过滤，排除那些普遍出现但无实际语义意义的词语，如“的”、“了”等。同时可以利用词频和逆文档频率（TF-IDF）来识别常见、过于普遍的词语，对其进行过滤或赋予适当的权重。
3. **噪音识别**：基于机器学习算法和模型，可以开发分类器，将文本分为“高质量”、“噪音”和“低质量”三类。利用训练好的模型对数据进行分类，挑选出高质量数据。
4. **文本聚类**：利用文本聚类技术将相似的文本数据分到一组，可以帮助发现并清除重复、相似或垃圾信息。
5. **主题建模**：通过主题建模技术，比如Latent Dirichlet Allocation (LDA)，寻找数据中的主题结构，能够帮助识别和过滤大量的噪音和低质量信息，只保留与关键主题相关的内容。
6. **人工审核**：在一些复杂情况下，结合人工审核，对无法通过算法清洗的数据进行人工干预和处理，帮助识别和排除低质量信息。

#### OpenWebText

是一个基于 Common Crawl 数据集构建而成的开放式文本语料库。该语料库旨在为自然语言处理和机器学习研究提供一个大规模、多样化的文本数据集。OpenWebText 中的数据来自于网页抓取

可以用于构建和训练语言模型、进行文本挖掘、分析主题建模等自然语言处理相关的任务。

#### PushShift.io

Pushshift.io 是一个提供对 Reddit 数据的搜索和访问 API 的平台。它收集和存储 Reddit 上的帖子、评论和其他数据，使研究人员和开发人员能够利用这些数据来进行分析、建模和其他数据驱动的工作。通过 Pushshift.io，用户能够以编程方式轻松地访问 Reddit 上的内容，从而进行各种形式的数据挖掘和分析。

以下是使用Pushshift.io API获取Reddit帖子的简单示例（使用Python）：

```
import requests

# 设置请求参数
params = {
    'subreddit': 'python',  # 指定 subreddit 名称
    'size': 10  # 指定要获取的帖子数量
}

# 发送 GET 请求
response = requests.get('https://api.pushshift.io/reddit/search/submission/', params=params)

# 检查响应状态
if response.status_code == 200:
    data = response.json()
    for post in data['data']:
        print(post['title'])  # 输出帖子标题
else:
    print("请求出错: %d" % response.status_code)
```

### 维基百科语料库

这个数据集包含了维基百科上的文章内容、编辑历史、页面链接等内容。

维基百科语料库通常被用于训练和评估各种自然语言处理模型，包括词向量模型、文本分类模型、命名实体识别模型等。

####  维基百科提供的API

维基百科提供了一个名为MediaWiki API的接口

MediaWiki API 是用于访问维基百科和其他MediaWiki站点数据的接口。它允许用户通过HTTP请求与维基百科的内容进行交互，并获取有关页面信息、搜索结果、页面历史记录等内容。

1. 获取页面内容：

```
copy codehttps://en.wikipedia.org/w/api.php?action=query&titles=Albert%20Einstein&prop=revisions&rvprop=content&format=json
```

这个示例中的请求将以JSON格式返回“Albert Einstein”页面的内容。

1. 搜索页面：

```
copy codehttps://en.wikipedia.org/w/api.php?action=query&list=search&srsearch=Albert%20Einstein&format=json
```

这个请求将返回关于“Albert Einstein”搜索结果的JSON数据。

1. 获取页面链接：

```
copy codehttps://en.wikipedia.org/w/api.php?action=query&titles=Albert%20Einstein&prop=links&format=json
```

这个请求将返回与“Albert Einstein”页面相关的链接列表。

### BigQuery 数据集

这些数据集包括许多不同的领域，如官方政府数据集、金融数据集、气象数据集、交通数据集以及其他公共领域数据集。（收费）

#### BigQuery 数据集API

BigQuery API 允许开发者通过编程方式与谷歌的 BigQuery 服务进行交互。它提供了执行查询、管理数据集和表格以及导出查询结果的功能。

以下是一些 BigQuery API 提供的功能：

1. **查询数据**：使用 SQL 查询语句对数据集中的信息进行分析和提取。
2. **管理数据**：创建、更新和删除数据表，以及管理数据集的元数据信息。
3. **加载和导出数据**：通过 API 将数据加载到 BigQuery 中，或者将查询结果导出到其他存储系统中。
4. **权限管理**：管理数据集和其它 BigQuery 资源的访问权限和安全设置。

用于大模型训练的 API 取决于您具体的用例和需求。常见的方法包括从 BigQuery 中检索数据后，将数据传输到用于训练的机器学习框架中，比如 TensorFlow、PyTorch 或 Scikit-learn。Google Cloud 也提供了许多与 BigQuery 结合使用的工具和服务，**比如 BigQuery ML，该服务允许用户直接在 BigQuery 中运行机器学习模型教程，而无需移动数据。**

执行查询：

```
copy codefrom google.cloud import bigquery

client = bigquery.Client()
query_job = client.query("""
    SELECT
      name, COUNT(*) as name_count
    FROM
      `bigquery-public-data.usa_names.usa_1910_current`
    GROUP BY
      name
    ORDER BY
      name_count DESC
    LIMIT 10""")
results = query_job.result()  # Waits for job to complete.
for row in results:
    print("{}: {}".format(row.name, row.name_count))
```

创建数据集：

```
copy codefrom google.cloud import bigquery

client = bigquery.Client()
dataset_id = 'your_project_id.your_dataset_id'
dataset = bigquery.Dataset(dataset_id)
dataset = client.create_dataset(dataset)  # Make an API request.
print("Created dataset {}.{}".format(client.project, dataset.dataset_id))
```

导出查询结果到 GCS（Google Cloud Storage）：

```
copy codefrom google.cloud import bigquery
job_config = bigquery.QueryJobConfig(destination="your_project_id.your_dataset_id.your_table_id")

query_job = client.query(
    "SELECT name FROM `bigquery-public-data.usa_names.usa_1910_current`",
    job_config=job_config)  # Make an API request.

query_job.result()  # Wait for the job to complete.

destination_table = client.get_table("your_project_id.your_dataset_id.your_table_id")
print("Exported {} rows to {}".format(destination_table.num_rows, destination_table.full_table_id))
```

### The Pile

The Pile（即“桩”数据集）是由EleutherAI创建的一个大型多模态数据集，其中包含了来自互联网的多种不同类型的数据，包括文本、图像和视频。这个数据集旨在成为用于预训练大型多模态模型的资源。

1. **数据清洗与预处理**：多模态数据集包含文本、图像和视频等多种数据类型。在处理之前，需要对数据进行清洗和预处理，确保数据格式的一致性并移除可能存在的噪音。
2. **数据加载**：根据模型训练的需求，将处理后的数据加载到模型训练环境中。如果是多模态模型，需要针对不同类型的数据准备不同的数据加载方法。
3. **模型训练**：使用加载的数据集对所选模型进行训练。这可能需要针对多模态数据集的特点进行调整，例如使用适当的损失函数来处理多种输入数据类型。
4. **评估和验证**：在训练完模型后，对模型进行评估与验证。根据任务的不同，这可能需要使用适当的指标和标准数据集来对模型的性能进行评估。
5. **模型优化**：依据评估结果对训练过的模型进行优化，并进行反复迭代，直至达到预期的性能水平。

在整个流程中，需要结合实际任务需求和模型类型，合理处理多模态数据集，并确保在模型训练和评估过程中考虑到数据的多样性和复杂性。

### 代码库的作用

* 加载预训练模型：
* 这些库允许用户轻松地加载和使用各种预训练的模型，例如GPT、BERT、RoBERTa等。这样，开发者可以使用这些强大的模型来执行各种自然语言处理任务，如文本生成、命名实体识别、情感分析等。

例如，使用Hugging Face Transformers加载GPT-3模型进行文本生成：

```
copy codefrom transformers import GPT2LMHeadModel, GPT2Tokenizer

tokenizer = GPT2Tokenizer.from_pretrained("gpt2")
model = GPT2LMHeadModel.from_pretrained("gpt2")
```

1. **微调模型**：这些库提供了微调预训练模型以适应特定任务或数据集的工具。使用预训练模型的参数作为起点，可以在特定任务上进一步调整模型以提高性能。比如，微调BERT模型用于情感分析任务。

```
copy codefrom transformers import BertForSequenceClassification, BertTokenizer

tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
model = BertForSequenceClassification.from_pretrained('bert-base-uncased')

# Fine-tune the model on a specific task
# ...
```

1. **部署模型**：这些库还提供了用于在生产环境中部署模型的工具和方法。这使得开发者可以将训练好的模型应用于实际应用中，例如在聊天机器人、智能客服系统中使用预训练模型。

```
copy code# 使用Hugging Face模型进行文本生成
output = model.generate(input_ids, max_length=50, num_return_sequences=3)
print(tokenizer.decode(output[0], skip_special_tokens=True))
```

以上代码示例展示了使用不同库加载、微调和部署预训练模型的基本方式。这些工具使得开发人员能够更轻松地利用现有的大型语言模型，并在各种自然语言处理任务中取得良好的性能。

### 各个代码库的优缺点

#### Hugging Face Transformers

**优点**：

- **提供了广泛的预训练模型选择**，包括各种Transformer-based模型（如GPT、BERT、RoBERTa等），方便开发者在各种任务中使用。
- **提供了易于使用的API**，使得加载、微调和使用模型非常简单。
- 拥**有庞大的社区支持**，用户可以方便地分享模型、专业知识和经验。
- 提供了模型共享和交流平台，让用户能够方便地分享、发现和使用各种模型和工具。

**缺点**：

- 有时对一些特定需求的**定制化处理可能会受到限制**，因为部分高阶的模型定制与调整可能需要更多底层的细节调整。

#### TensorFlow 和 PyTorch

**优点**：

- TensorFlow 和 PyTorch 是两个流行的深度学习框架，**提供了丰富的工具和库**，支持构建和训练各种类型的神经网络模型。
- 提供了丰富的社区支持和大量的文档资源，使得开发者可以获得广泛的技术支持和帮助。

**缺点**：

- 相对于Hugging Face Transformers，这些框架可能需要更多的工程师技能和领域知识，因为搭建和训练模型需要**更多的手动实现和配置**。

#### Fairseq

**优点**：

- Fairseq提供了用于序列到序列学习的强大框架，并已在许多研究工作中取得了成功。
- 适用于深度学习研究人员和工程师，提供了一些高级和前沿特性。

**缺点**：

- 相对于Hugging Face Transformers而言，它的用户界面和易用性可能略逊一筹，可能需要更多的技术背景和钻研。

#### DeepSpeed

**优点**：

1. **分布式训练**：DeepSpeed 提供了用于高效分布式训练的工具和库，能够加速大型语言模型的训练，特别是针对包含数十亿甚至数千亿参数的模型。
2. **模型优化**：其内置了一系列模型压缩、减少内存占用、混合精度训练等优化技术，可以帮助开发者利用有限的硬件资源训练更大、更复杂的模型。
3. **易于使用**：DeepSpeed 的设计旨在提供易于使用的接口和工具，帮助开发者在分布式设置下更轻松地进行模型训练。

**缺点**：

1. **学习曲线**：初学者可能需要一定时间来理解并熟练使用 DeepSpeed 中的各种优化工具和技术，因为其中的一些概念对于不熟悉分布式训练和模型优化的开发者来说可能具有一定的学习曲线。
2. **定制化需求**：对于部分特定定制化需求，DeepSpeed 可能并不提供相应的工具或库，需要开发者自行进行扩展或定制。



#### Megatron-LM

**优点**：

1. **多GPU训练**："megatron-lm" 提供了优化的分布式训练策略，能够高效地利用多个GPU进行训练，从而加速大型语言模型的训练过程。
2. **大规模模型**：该项目专注于训练大规模语言模型，并提供了相应的工具和优化技术，使得开发者能够处理含有数十亿甚至上百亿参数的模型。
3. **性能优化**："megatron-lm" 针对分布式训练做了深度优化，以提高训练效率和模型收敛速度。

**缺点**：

1. **学习曲线**：对于初学者来说，"megatron-lm" 可能需要一定时间来理解其分布式训练策略以及配置和使用多GPU系统的细节。
2. **定制化需求**：某些特定的定制化需求可能需要开发者自己进行扩展和定制，因为并非所有的特定场景都能够在项目中找到相应的工具和解决方案。

#### JAX

**优点**：

1. **自动微分**：JAX 提供了自动微分的功能，可用于梯度计算，这使得在深度学习和优化问题中更容易实现自定义的梯度下降算法和新颖的神经网络架构。
2. **XLA 编译器**：JAX 使用 XLA（加速线性代数）进行即时编译，从而能够提高数值计算的效率，特别是在使用GPU和TPU时。
3. **函数式编程**：JAX 基于函数式编程范式，鼓励不可变值和纯函数，使得代码更易于并行化、测试和推理。

**缺点**：

1. **学习曲线**：对于那些不熟悉函数式编程和即时编译器的开发者来说，JAX 有一定的学习曲线，尤其是在处理复杂的神经网络架构和优化概念时。
2. **社区和生态系统**：相较于其他更成熟的深度学习框架（如TensorFlow和PyTorch），JAX 的社区和生态系统相对较小，可能在某些方面缺乏一些常见工具和组件的支持。



#### Colossal-AI

参考以下文章

* [采访](https://www.zhihu.com/question/512781791)

* **[Github](https://github.com/hpcaitech/ColossalAI/blob/main/docs/README-zh-Hans.md#为何选择-Colossal-AI)**

#### BMTrain

BMTrain 是一个高效的大模型训练工具包，可以用于训练数百亿参数的大模型。BMTrain 可以在分布式训练模型的同时，能够保持代码的简洁性。

解决**显存占用** 问题

* [Github](https://github.com/OpenBMB/BMTrain/blob/main/README.md)

* **[采访](https://zhuanlan.zhihu.com/p/531507490)**

#### FastMoE

它旨在通过有效地利用混合专家模型（Mixture of Experts, MoE）加速训练，并且能够扩展到大规模的模型和数据集。这使得 FastMoE 特别适用于大规模语言模型和其他复杂的神经网络应用。

FastMoE 的主要优点包括高效的模型并行化训练，适用于大型模型的分布式训练，以及提供了一些用于减少内存占用和提高训练效率的优化技术。这使得 FastMoE 成为在大规模模型训练中提升效率和加速收敛的有力工具。



# 百川大模型

[API接口文档](https://platform.baichuan-ai.com/docs/api)

POST请求，URL： `https://api.baichuan-ai.com/v1/chat/completions`

百川大模型在文本理解以及推理方面优异于GPT的表现。

对于训练百川大模型，分为五大步：

## 1. 构建优质的训练集



预训练数据的质量、多样性、规模、代表性以及公平性和偏见等因素对模型性能有着决定性影响

Baichuan-7B模型在数据方面做了以下处理

（以下为百川模型训练文章精炼）

* 数据规模和多样性：baichuan-7B的原始数据包括开源的中英文数据和自行抓取的中文互联网数据，在过滤后也有1.2T的量级，同时保证了数据的多样性
* 数据的公平性和偏见：去掉数据中的噪声和有害数据，避免输出有害性内容
* 数据质量和代表性：基于启发式规则和质量模型打分，对原始数据集进行篇章和句子粒度的过滤，减少数据中重复的pattern和不具有代表性的数据。

#### 百川训练的清理

数据的频率依赖于聚类和去重。百川构建了一个支持LSH-like特征的大规模去重和聚类系统

对匹配去重 29.89% 数据，启发式方法去除 1.77%，句子级别的质量过滤 3%，句子级别和段落级别去重 14.47%，文档级别去重 19.13%

这一步很重要，我们的训练集只做到了第一步，直接拿这个没有清晰完全的训练集训练数据会导致模型后期出现幻觉的问题。

## 2. 制定合适的分词规则

（以下为百川模型训练文章精炼）

baichuan-7B采用了SentencePiece中的Byte-Pair Encoding(BPE)作为**分词算法**，并且进行以下优化：

* 使用2000万条中英文为主的多语言语料训练分词模型，提高中文的压缩率
* 对数字的每一位单独分开，避免出现数字不一致的问题
* 对于特殊符号等，支持UTF-8的编码，实现未知词全覆盖

Baichuan在分词器中压缩率相当的情况下，训练和推理的效率更高，明显优于LLaMA、Falcon等开源模型

#### 百川的分词器设计

利于高效推理的高压缩率和适当大小的词汇表，以确保每个词嵌入的充分训练。百川将词汇表从64000扩展到125696，旨在计算效率和模型性能之间取得平衡。

- 使用 SentencePiece 的字节对编码（BPE）来对数据进行 tokenize 处理
- 没有对输入文本应用任何规范化，并且没有像 Baichuan 1 那样添加虚拟前缀
- 将数字拆分成单个数字以更好地编码数字数据
- 为了处理包含额外空格的代码数据，我们向分词器添加了仅包含空格的 token。字符覆盖率设置为 0.9999，罕见字符回退到 UTF-8 字节
- 将最大 token 长度设置为 32，以考虑长的中文短语。Baichuan2 分词器的训练数据来自 Baichuan2 预训练语料库，包括对于代码示例和学术论文更多采样以提高覆盖率

## 3. 打造良好的模型结构

（以下为百川模型训练文章精炼）

* **位置编码**

  初步实验中，位置嵌入的选择对模型性能影响不大

  > 大型模型中的位置编码是一种用于为输入序列中的每个位置提供独特编码的方法。这是因为**在自然语言处理任务中，单词的顺序通常很重要**，因此模型需要能够区分不同位置的单词。为了实现这一点，位置编码会为每个位置分配一个特定的向量值，以在输入中表示位置信息。这有助于模型更好地理解输入文本的结构和顺序。常见的方法包括使用正弦函数和余弦函数来计算位置编码，或者使用可学习的参数来动态地表示位置信息。这些技术有助于模型更好地理解输入文本中单词的位置。

  加入位置编码用于表征词之间的位置关系

  在原始的Transformer模型中引入了一种预定义的正弦函数作为位置编码。这种方法会为每个位置生成一个固定向量，该向量的维度跟词嵌入向量相同。

  其优点在于，它可以处理任意长度的序列，不需要额外的学习过程，并且对于相对位置关系有一定的编码能力。

  对于每个位置，模型都有一个对应的嵌入向量，这个向量会在模型训练的过程中学习和优化。

  该方法的优点在于，它可以根据具体的任务和数据集学习位置信息。然而，它的一大缺点是，由于位置嵌入数量固定，因此模型可能无法处理超过预先设定数量位置的序列。

* **注意力层**

  > 注意力层是深度学习中常见的组件，尤其在自然语言处理领域中的Transformer模型中被广泛使用。它允许模型将不同位置的信息聚合在一起以便更好地理解输入。在注意力层中，对于每对输入中的元素，模型会计算它们之间的关联程度，然后根据这些关系的强度对输入进行加权，以便更好地理解不同元素之间的相互作用。这一过程有助于模型更好地理解句子中不同单词之间的重要性和联系。

* **前馈神经网络**

  > 前馈神经网络（Feedforward Neural Network）是一种最基本的人工神经网络结构。它的特点是信息在网络中单向传递，从输入层经过一层又一层的隐藏层到输出层，没有形成循环。典型的前馈神经网络由输入层、若干个隐藏层和输出层组成，每个隐藏层通过激活函数（如ReLU、Sigmoid等）对输入信号进行加权求和并输出至下一层。这种网络结构适用于许多监督学习任务，如分类和回归。

## 4. 优化训练策略

（以下为百川模型训练文章精炼）

* baichuan-7B采用了算子技术（例如Flash-Attention和NVIDIA apex的RMSNorm），降低显存消耗。
* 采用算子切分技术，对部分计算算子进行切分，这样在通信时会以更加高效的nccl操作进行通信。
* 使用了混合精度技术，在不牺牲模型准确性的前提下，加速了计算过程。
* 采用了通信优化技术：利用拓扑感知的集合通信算法避免网络拥堵问题
* 卡数自适应设置bucket size，提高带宽利用率
* 根据模型和集群环境，调优通信原语的触发时机，从而实现计算和通信的有效重叠。
* 自研了一种名为训练容灾的技术：通过训练平台和训练框架的联合优化以及IaaS+PaaS的应用，baichuan-7B实现了分钟级的故障定位和任务恢复，从而保证了训练的稳定性和效率。

## 5. 合适的模型评价方法

（以下为百川模型训练文章精炼）

* C-Eval数据集是最全面的中文基础模型评测数据集，涵盖了52个学科和四个难度的级别；

* Gaokao是以中国高考题作为评测大语言模型能力的数据集，用以评估模型的语言能力和逻辑推理能力；

* AGIEval旨在评估模型在中文环境下，认知和解决问题等相关的任务的能力；
* MMLU是一个包含57个多选任务的英文评测数据集，涵盖了初等数学、美国历史、计算机科学、法律等，难度覆盖高中水平到专家水平，是目前主流的LLM评测数据集。

## 幻觉处理优化

[大模型的幻觉问题调研: LLM Hallucination Survey - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/642648601)

幻觉是指在没有任何事实依据的情况下，模型产生不准确的信息。

通常解决这一类问题有两种方式，一种是不断增加模型的数据规模、提升数据质量；另一种是通过调用搜索等外部工具让模型能够获取实时信息。

baichuan将获取的数据按照低高质量进行划分，确保模型只使用高质量的数据进行预训练。

对于判断模型的生成数据的准确性即模型产生幻觉的程度，baichuan使用[FacTool](https://github.com/GAIR-NLP/factool)进行数据评测，检查大模型生成内容的事实准确性。

## 重复生成现象

大模型建模概率一般是一个条件概率，如下：

p(xt|x1,x2,...,xt−1)p(x_t|x_1, x_2,...,x_{t-1})p(x_t|x_1, x_2,...,x_{t-1}) 

即，大模型通过前t-1个token作为条件，来预测第t个token的是哪一个，当你的前面的条件文本过长时，大模型的输出的几个短文本会被原始的很长的条件文本淹没，继续预测下一个token的话，在模型看起来可能条件仍然是差不多的（因为对于很长的文本来说几乎没发生变化，只新增了非常短的文本），此时如果使用greedy search，只选择概率最大的一个token，模型极大可能会将前面已经生成的短文本重新预测成概率最大的文本，以此类推，会一直重复下去。





参考资料：[百川大模型研发技术报告](https://zhuanlan.zhihu.com/p/657487567)     [百川模型训练](https://zhuanlan.zhihu.com/p/642110905)     [百川模型训练细节](https://zhuanlan.zhihu.com/p/656451863)     [百川2-53B搜索增强-开放API](https://zhuanlan.zhihu.com/p/658469464)



可参考资料:[微调百川Baichuan-13B保姆式教程，手把手教你训练百亿大模型 - 知乎 ](https://zhuanlan.zhihu.com/p/643950663)

[LLM - 大模型技术报告与训练细节 By Baichuan2_BIT_666的博客-CSDN博客](https://blog.csdn.net/BIT_666/article/details/133035120)



